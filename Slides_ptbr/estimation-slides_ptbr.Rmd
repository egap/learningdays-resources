---
title: "Parâmetros a Serem Estimados e Estimadores"
author: "Tradução: Júlia Papa"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  beamer_presentation:
    keep_tex: yes
    slide_level: 2
    toc: yes
  revealjs::revealjs_presentation:
    center: no
    fig_caption: yes
    highlight: pygments
    pandoc_args: --toc
    reveal_options:
      chalkboard:
        theme: whiteboard
        toggleNotesButton: no
      previewLinks: yes
      slideNumber: yes
    reveal_plugins:
    - notes
    - search
    - chalkboard
    self_contained: no
    smart: no
    theme: default
    transition: fade
bibliography: ../learningdays-book.bib
header-includes: |
   \setbeamertemplate{footline}{\begin{beamercolorbox}{section in head/foot}
   \includegraphics[height=.5cm]{../Images/egap-logo.png} \hfill
   \insertframenumber/\inserttotalframenumber \end{beamercolorbox}}
   \usepackage{makecell}
   \usepackage{tikz}
   \usepackage{tikz-cd}
   \usetikzlibrary{arrows,automata,positioning,trees,babel}
   \usepackage{textpos}
   \usepackage{booktabs,multirow}
link-citations: yes
colorlinks: yes
biblio-style: apalike
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
# Load all the libraries we need
library(here)
library(tidyverse)
library(kableExtra)
library(DeclareDesign)
library(estimatr)
library(styler)
library(rlang)
library(vctrs)
library(ggplot2)
library(dplyr)
library(cli)
```

# Pontos-Chave

## Pontos-Chave sobre Estimação I

- Um efeito causal, $\tau_i$, é uma comparação de resultados potenciais não observados para cada unidade $i$: exemplos: $\tau_{i} = Y_{i}(T_{i}=1) - Y_{i}(T_{i}=0)$ ou $\tau_{i} = \frac{Y_{i}(T_{i}=1)}{ Y_{i}(T_{i}=0)}$.

- Para aprender sobre $\tau_{i}$, podemos tratar $\tau_{i}$ como uma
    **estimativa** ou quantidade de interesse a ser estimada (discutido
    aqui) ou como uma quantidade de interesse a ser hipotetizada (sessão
    sobre testes de hipóteses).

- Muitos focam no **efeito médio do tratamento (ATE)**,
    $\bar{\tau}=\sum_{i=1}^n\tau_{i}$, em parte porque é fácil de
    **estimar** e sua variabilidade de experimento para experimento é
    fácil de calcular.

## Pontos-Chave sobre Estimação II

-   A chave para estimação em inferência causal é escolher um
    estimador que te ajude a aprender sobre sua questão teórica ou
    de formulação de política pública. Assim, pode-se usar o ATE, mas outros       estimandos comuns
    incluem ITT, LATE/CACE, ATT, ou ATE para algum subgrupo (ou mesmo
    uma diferença de efeitos causais entre grupos) ou até mesmo efeitos
    de quantil.

-   Um **estimador** é uma receita para calcular o valor aproximado
    de uma estimativa. Por exemplo, a diferença das médias observadas para
    $m$ unidades tratadas é um estimador de $\bar{\tau}$:
    $\hat{\bar{\tau}} = \frac{\sum_{i=1}^n (T_i Y_i)}{m} - \frac{\sum_{i=1}^n ( ( 1 - T_i)Y_i)}{(n-m)}$.

## Pontos-Chave sobre Estimação III

- O **erro padrão** de um estimador em um experimento randomizado
    resume como as estimativas variariam caso o experimento fosse
    repetido. Diferentes randomizações produzirão diferentes valores do
    mesmo estimador voltados para a mesma estimativa. Um **erro
    padrão** resume essa variabilidade em um estimador.

- Usamos o **erro padrão** e suposições sobre a distribuição de uma
    estatística de teste para produzir **intervalos de confiança** e
    **p-valores**: para que possamos começar com um estimador, tratá-lo
    como uma estatística de teste e terminar em um teste de hipóteses.

- Um **intervalo de confiança** de $100 \times (1 - \alpha)$% é uma coleção de hipóteses que não podem ser rejeitadas no nível $\alpha$. Costumamos relatar intervalos de confiança contendo hipóteses sobre valores de nossa estimativa e usamos nosso estimador como uma estatística de teste: pelo menos é o que a maioria das tabelas de regressão relata.

## Pontos-Chave sobre Estimação IV

- Os estimadores devem:

    - Evitar erros sistemáticos em sua estimativa (ser imparciais);

    - Variar pouco em suas estimativas de experimento para experimento (ser precisos ou eficientes); e

    - Idealmente convergir para a estimativa à medida que usam cada vez mais informações (ser consistentes).

## Pontos-Chave sobre Estimação V

-   **Analisar conforme se randomiza** no contexto de estimação
    significa que:
    (1) nossos erros padrão devem medir a variabilidade decorrente da
        randomização (não necessariamente a variabilidade decorrente da
        amostragem, a menos que saibamos como a amostra foi selecionada
        da população) e

    (2) nossos estimadores devem visar estimativas definidas em termos de
        resultados potenciais.
-   Não **controlamos** as covariáveis de contexto quando analisamos dados
    de experimentos randomizados. Mas as covariáveis podem tornar nossa
    estimativa mais **precisa**. Isso é chamado **ajuste de
    covariância** (ou ajuste de covariáveis). O **ajuste de
    covariância** em experimentos randomizados difere do ajuste de
    covariância em estudos observacionais.

# Revisão

## Revisão: Efeitos Causais

Revisão: Inferência causal refere-se à comparação de resultados
potenciais não observados e fixos.

Por exemplo:

-   o resultado potencial ou possível para a unidade $i$ quando
    atribuída ao tratamento, $T_i=1$ ´r $Y_{i}(T_{i}=1)$.
-   o resultado potencial ou possível para a unidade $i$ quando
    atribuída ao grupo de controle, $T_i=0$ é $Y_{i}(T_{i}=0)$.

A atribuição de tratamento, $T_i$, tem um efeito causal na unidade $i$,
que chamamos de $\tau_i$, se $Y_{i}(T_{i}=1) - Y_{i}(T_{i}=0) \ne 0$ ou
$Y_{i}(T_{i}=1) \ne Y_{i}(T_{i}=0)$.

# Estimativas, estimadores e médias

## Como podemos aprender sobre efeitos causais a partir de dados observados?

1.  Lembrete: podemos testar hipóteses sobre o par de resultados
    potenciais $\{ Y_{i}(T_{i}=1), Y_{i}(T_{i}=0) \}$.

2.  Podemos **definir estimativas** em termos de
    $\{ Y_{i}(T_{i}=1), Y_{i}(T_{i}=0) \}$ ou $\tau_i$, **desenvolver
    estimadores** para essas estimativas e, em seguida, calcular
    **estimações** e **erros padrão** para esses estimadores.

## Um estimador e estimativas comuns: O efeito médio do tratamento e a diferença das médias.

Suponha que estamos interessados no ATE, ou
$\bar{\tau}=\sum_{i=1}^n \tau_{i}$. Qual é um bom estimador?

Dois candidatos:

1.  A diferença das médias:
    $\hat{\bar{\tau}} = \frac{\sum_{i=1}^n (T_i Y_i)}{m} - \frac{\sum_{i=1}^n ( ( 1 - T_i) Y_i)}{n-m}$.

2.  Uma diferença de médias após a seleção do valor máximo de $Y_i$ (uma
    espécie de média "winsorizada" para evitar que valores extremos
    exerçam muita influência sobre nosso estimador --- para aumentar
    *precisão*).

    Como saber qual estimador é o melhor para nosso design de pesquisa
    em particular?

    Vamos simular!

## Passo 1 da simulação: criar dados com um ATE conhecido

Observe que precisamos conhecer os resultados potenciais e a atribuição
de tratamento para saber se nosso estimador proposto funciona bem.

```{r echo=FALSE}

## Primeiro, vamos criar alguns dados
##  y0 é o potencial resultado do grupo de controle
N <- 10
y0 <- c(0, 0, 0, 1, 1, 3, 4, 5, 190, 200)
## Cada unidade tem seu próprio efeito de tratamento
tau <- c(10, 30, 200, 90, 10, 20, 30, 40, 90, 20)
## y1 é o resultado potencial do grupo de tratamento
y1 <- y0 + tau
## Z é a atribuição de tratamento (observe que estamos usando Z em vez de T)
set.seed(12345)
block <- c("a", "a", "a", "a", "a", "a", "b", "b", "b", "b")
Z <- c(0, 0, 0, 0, 1, 1, 0, 0, 1, 1)
## Y são os resultados observados
Y <- Z * y1 + (1 - Z) * y0
## Os dados
dat <- data.frame(Z = Z, y0 = y0, y1 = y1, tau = tau, b = block, Y = Y)
## dat <- dat[,c("Z","y0","y1")]
```

````{=tex}
\begin{center}
```{r}
kableExtra::kable(dat[, c("Z", "y0", "y1")])
```
\end{center}
````

```{r ate, echo=FALSE, results="markup", message=TRUE}
ATE <- with(dat, mean(y1 - y0))
message("The true ATE is ", ATE)
```

Na realidade, observaríamos apenas um dos resultados potenciais.

Observe que cada unidade tem seu próprio efeito de tratamento.

## Primeiro, crie dados falsos

A tabela no slide anterior foi gerada em R com o seguinte código:

```{r echo=TRUE}
# Temos dez unidades
N <- 10
# y0 é o resultado potencial do grupo controle
y0 <- c(0, 0, 0, 1, 1, 3, 4, 5, 190, 200)
# Cada unidade tem seu próprio efeito de tratamento
tau <- c(10, 30, 200, 90, 10, 20, 30, 40, 90, 20)
# y1 é o resultado potencial do grupo tratamento
y1 <- y0 + tau
# Dois blocos, a e b
block <- c("a", "a", "a", "a", "a", "a", "b", "b", "b", "b")
# Z é a atribuição de tratamento (Z em vez de T no código)
Z <- c(0, 0, 0, 0, 1, 1, 0, 0, 1, 1)
# Y é o resultado observado
Y <- Z * y1 + (1 - Z) * y0
# Os dados
dat <- data.frame(Z = Z, y0 = y0, y1 = y1, tau = tau, b = block, Y = Y)
set.seed(12345)
```

## Usando DeclareDesign

DeclareDesign representa desenhos de pesquisa em poucos passos, como
mostrado abaixo

```{r dd1, echo=TRUE}
# Selecione apenas os resultados potenciais para o tratamento e controle dos nossos dados fictícios
# dados fictícios
small_dat <- dat[, c("y0", "y1")]

# DeclareDesign primeiro solicita que você identifique a população
pop <- declare_population(small_dat)
N <- nrow(small_dat)

# 2 unidades atribuídas ao tratamento; o padrão é atribuição aleatória simples com probabilidade 0,5
trt_assign <- declare_assignment(Z = conduct_ra(N = N, m = 2), legacy = FALSE)

# Y observado é y1 se Z = 1 e y0 se Z = 0
pot_out <- declare_potential_outcomes(Y ~ Z * y1 + (1 - Z) * y0)

# especifica variáveis de resultado e atribuição
reveal <- declare_reveal(Y, Z)

# o objeto de design de pesquisa básico inclui esses quatro objetos
base_design <- pop + trt_assign + pot_out + reveal
```

## Usando DeclareDesign: criando dados falsos

Por padrão, o DeclareDesign renomeia `y0` e `y1` para `Y_Z_0` e `Y_Z_1`:

```{r echo=TRUE}
## Uma simulação é uma atribuição aleatória de tratamento.
sim_dat1 <- draw_data(base_design)

## Dados simulados (apemas as 6 primeiras linhas)
head(sim_dat1)
```

## Usando DeclareDesign: definir estimativa e estimadores

Aqui estamos definindo uma função que calcula a diferença média dos
potenciais resultados para os indivíduos tratados e não tratados:

```{r dd2, echo=TRUE}
## A estimativa
estimandATE <- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))

## O primeiro estimador é a diferença entre médias
diff_means <- declare_estimator(Y ~ Z,
  inquiry = estimandATE,
  model = lm_robust, se_type = "classical", label = "Diff-Means/OLS"
)
```

## Usando DeclareDesign: definindo estimativas e estimadores

```{r dd2a, echo=TRUE}
## O segundo estimador é a diferença de médias com codificação superior
diff_means_topcoded_fn <- function(data) {
  data$rankY <- rank(data$Y)
  ## Codifique o valor máximo de Y como o segundo maior valor de Y
  data$newY <- with(
    data,
    ifelse(rankY == max(rankY), Y[rankY == (max(rankY) - 1)], Y)
  )
  obj <- lm_robust(newY ~ Z, data = data, se_type = "classical")
  res <- tidy(obj) %>% filter(term == "Z")
  return(res)
}
diff_means_topcoded <- declare_estimator(
  handler = label_estimator(diff_means_topcoded_fn),
  inquiry = estimandATE, label = "Top-coded Diff Means"
)
```

## Usando DeclareDesign: definindo estimativas e estimadores

Aqui mostramos como os estimadores do DD funcionam usando nossos dados
simulados.

```{r dd3, echo=TRUE}
## Demonstrando que a estimativa funciona:
estimandATE(sim_dat1)

## Demonstrando que os estimadores estimam
## Estimador 1 (diferença nas médias)
diff_means(sim_dat1)[-c(1, 2, 10, 11)]

## Estimador 2 (diferença nas médias da parte superior)
diff_means_topcoded(sim_dat1)[-c(1, 2, 10, 11)]
```

## Então vamos simular com uma randomização

Lembre-se do ATE verdadeiro:

```{r trueATE, echo=TRUE}
trueATE <- with(sim_dat1, mean(y1 - y0))
with(sim_dat1, mean(Y_Z_1 - Y_Z_0))
```

Em um experimento (uma simulação dos dados), aqui estão as estimações
simples:

```{r echo=TRUE}
## Duas maneiras de calcular o estimador de diferença de médias:
est_diff_means_1 <- with(sim_dat1, mean(Y[Z == 1]) - mean(Y[Z == 0]))
est_diff_means_2 <- coef(lm_robust(Y ~ Z,
  data = sim_dat1,
  se = "classical"
))[["Z"]]
c(est_diff_means_1, est_diff_means_2)
```

## Então vamos simular com uma randomização

Em um experimento (uma simulação dos dados) aqui estão as estimativas
após a codificação superior (top-coding):

```{r echo=TRUE}
## Duas formas de calcular o estimador de diferença de médias truncadas são mostradas abaixo:
sim_dat1$rankY <- rank(sim_dat1$Y)
sim_dat1$Y_tc <- with(sim_dat1, ifelse(rankY == max(rankY),
  Y[rankY == (max(rankY) - 1)], Y
))
est_topcoded_1 <- with(sim_dat1, mean(Y_tc[Z == 1]) - mean(Y_tc[Z == 0]))
est_topcoded_2 <- coef(lm_robust(Y_tc ~ Z,
  data = sim_dat1,
  se = "classical"
))[["Z"]]
c(est_topcoded_1, est_topcoded_2)
```

## Em seguida, simule uma aleatorização diferente e estime o ATE com os mesmos estimadores

Agora, calcule sua estimação com os mesmos estimadores usando uma
aleatorização **diferente**. Note que as respostas diferem. Os
estimadores estão estimando a *mesma estimativa*, mas agora têm uma
randomização diferente para trabalhar.

```{r echo=TRUE}
# faça outra atribuição aleatória do tratamento no DeclareDesign
# isso produz um novo conjunto de dados simulados com uma atribuição aleatória diferente
sim_dat2 <- draw_data(base_design)
# o primeiro estimador (diferença nas médias)
coef(lm_robust(Y ~ Z, data = sim_dat2, se = "classical"))[["Z"]]
# o segundo estimador (diferença nas médias truncada (top-coded))
sim_dat2$rankY <- rank(sim_dat2$Y)
sim_dat2$Y_tc <- with(sim_dat2, ifelse(rankY == max(rankY),
  Y[rankY == (max(rankY) - 1)], Y
))
coef(lm_robust(Y_tc ~ Z, data = sim_dat2, se = "classical"))[["Z"]]
```

## Como nossos estimadores se comportam em geral para este desenho experimental?

Nossas estimativas variam entre as randomizações. Nossos dois
estimadores variam da mesma maneira?

```{r diagnose, echo=TRUE, cache=TRUE}
## Combine em um objeto de design DeclareDesign
## Isso envolve o design base, estimativa e dois estimadores
design_plus_ests <- base_design + estimandATE + diff_means +
  diff_means_topcoded
## Execute 100 simulações (reatribuições de tratamento) e
## aplique os dois estimadores (diff_means e diff_means_topcoded)
diagnosis1 <- diagnose_design(design_plus_ests,
  bootstrap_sims = 0, sims = 100
)
sims1 <- get_simulations(diagnosis1)
head(sims1[, -c(1, 2, 3, 6, 9, 13)])
```

## Como se comportam nossos estimadores em geral para este design?

Nossas estimativas variam entre diferentes randomizações. Nossos dois
estimadores variam da mesma maneira? Como devemos interpretar este
gráfico?

```{r sim_plot, out.width=".8\\textwidth"}
sim_plot <- ggplot(sims1, aes(y = estimate, x = estimator, color = estimator)) +
  geom_boxplot() +
  geom_hline(yintercept = trueATE) +
  geom_point(aes(group = estimator)) +
  geom_text(aes(x = .5, y = trueATE), label = "ATE Verdadeiro", inherit.aes = FALSE) +
  theme(text = element_text(size = 20))
print(sim_plot)
```

## Qual estimador está mais próximo da verdade?

Uma maneira de escolher entre estimadores é escolher aquele que está
**próximo da verdade** sempre que o usamos --- independente da
randomização específica.

Um estimador "não enviesado" é aquele para o qual a **média das
estimativas em designs repetidos** é igual à verdade (ou
$E_R(\hat{\bar{\tau}})=\bar{\tau}$). Um estimador não enviesado não
tem "erro sistemático", mas não garante proximidade com a verdade.

Outra medida de proximidade é o **erro quadrático médio** (RMSE), que
registra as distâncias quadradas entre a verdade e as estimativas
individuais.

Qual estimador é melhor? (Um está mais próximo da verdade em média
(RMSE) e é mais preciso. O outro não tem erro sistemático - é não enviesado.)

```{r}
kableExtra::kable(reshape_diagnosis(diagnosis1, select = c("Estimator", "Bias", "RMSE", "SD Estimate", "Power"))[, -c(1, 2, 4, 5, 6)])
```

## Estimadores sem viés e com viés

Resumo:

-   Temos a opção de escolher tanto estimativas quanto estimadores.

-   Um bom estimador funciona bem independentemente da randomização
    particular de um determinado projeto. E por *funcionar bem* pode querer 
    significar tanto "sem viés" e/ou "baixo mse" (ou "consistente" --- o que
    quer dizer cada vez mais próximo da verdade conforme o tamanho da
    amostra aumenta).

-   Podemos aprender como um determinado estimador se comporta em um
    estudo dado usando simulação.

# Randomização em blocos

```{r setup_block_dat, echo=FALSE}
library(randomizr)
# Temos 10 unidades
N_blocked <- 100
# 3 blocos, a, b e c 
block <- c(rep("a", 10), rep("b", 30), rep("c", N_blocked - (10 + 30)))
set.seed(12345)
# Z é a atribuição de tratamento (Z em vez de T no código).
Z_blocked <- block_ra(blocks = block, block_m = c(5, 5, 5))
dat_blocked <- data.frame(Z = Z_blocked, b = block)
# y0 é o potencial resultado para o grupo de controle
dat_blocked <- dat_blocked %>%
  group_by(b) %>%
  mutate(
    nb = n(),
    y0 = rgeom(nb, prob = .1 * (nb / N)),
    tau = sd(y0) * runif(nb, min = 0, max = max(y0)),
    y1 = y0 + tau,
    Y = Z * y1 + (1 - Z) * y0
  )
```

## Experimentos com blocos aleatórios são uma coleção de mini-experimentos.

Qual é o estimador **ATE** em um experimento com blocos aleatórios?

Se pensarmos no ATE a nível de unidade como: então podemos expressá-lo
de forma equivalente usando o ATE no bloco $j$ é $ATE_j$ , da seguinte
forma:

$$
ATE = \frac{1}{J}\sum^J_{j=1} \sum^{N_j}_{i=1} \frac{y_{i,1} - y_{i,0}}{N_j}  = \sum^J_{j=1} \frac{N_j}{N} ATE_j
$$

E seria natural estimar essa quantidade substituindo o que podemos
calcular:

$\widehat{ATE} = \displaystyle\sum^J_{j=1} \frac{N_j}{N} \widehat{ATE}_j$

## Experimentos randomizados em blocos são uma coleção de mini-experimentos

E poderíamos *definir* o erro padrão do estimador também apenas pela
média dos erros padrões dentro do bloco (se nossos blocos forem grandes
o suficiente):$SE(\widehat{ATE}) = \sqrt{\sum^J_{j=1} (\frac{N_{j}}{N})^2SE^2(\widehat{ATE}_j)}$

## Estimando o ATE em experimentos com randomização em bloco

Uma abordagem de estimação simplesmente substitui $ATE_j$ por
$\widehat{ATE}$ acima:

```{r br1, echo=TRUE}
with(dat_blocked, table(b, Z))
```

Nós temos 10 unidades no bloco `a`, 5 das quais foram atribuídas ao
tratamento, e 30 unidades no bloco `b`, 5 das quais foram atribuídas ao
tratamento.

## Estimando o ATE em experimentos com randomização em blocos

Aqui vemos que o verdadeiro $ATE$ pode ser definido e calculado usando
observações individuais diretamente ou como uma média ponderada pelo
tamanho do bloco do verdadeiro $ATE_j$ dentro do bloco.

```{r br2, echo=TRUE}
datb <- dat_blocked %>%
  group_by(b) %>%
  summarize(
    nb = n(), pb = mean(Z), estateb = mean(Y[Z == 1]) - mean(Y[Z == 0]),
    ateb = mean(y1 - y0), .groups = "drop"
  )
datb
## Observe o verdadeiro ATE por bloco:
with(dat_blocked, mean(y1 - y0))
## Esta é outra maneira de calcular o verdadeiro ATE
with(datb, sum(ateb * (nb / sum(nb))))
```

## Estimando o ATE em experimentos com blocos aleatorizados

Uma abordagem é estimar o ATE geral usando pesos de tamanho dos blocos,
simplesmente substituindo $ATE_j$ com $\widehat{ATE}$ acima:

```{r br3, echo=TRUE}
## Mostrando que difference_in_means usa o peso do tamanho do bloco.
e1 <- difference_in_means(Y ~ Z, blocks = b, data = dat_blocked)
e2 <- with(datb, sum(estateb * (nb / sum(nb))))
c(coef(e1)[["Z"]], e2)
```

## Estimando o ATE em experimentos com aleatorização em blocos

Observe que isso **não** é o mesmo que nenhum dos seguintes:

```{r br4, echo=TRUE}
## Ignorando os blocos
e3 <- lm(Y ~ Z, data = dat_blocked)
coef(e3)[["Z"]]

## Com efeitos fixos de blocos
e4 <- lm(Y ~ Z + b, data = dat_blocked)
coef(e4)[["Z"]]
```

Como eles diferem? (O primeiro ignora os blocos. O segundo usa um
conjunto diferente de pesos criados pelo uso de variáveis "fixas",
"indicadoras" ou "dummy".)

## Qual estimador devemos usar?

Agora temos três estimadores, cada um com uma estimativa diferente
(imaginando que todos visam o mesmo estimador):

```{r echo=TRUE}
c(coef(e1)[["Z"]], coef(e3)[["Z"]], coef(e4)[["Z"]])
```

Para este desenho, qual estimador devemos usar? Podemos configurar uma
simulação DeclareDesign para descobrir.

```{r blockdd0, cache=TRUE, echo=TRUE}
## declarar um novo design básico que inclui o indicador de bloco b
base_design_blocks <-
  # identificar a população
  declare_population(dat_blocked[, c("b", "y0", "y1")]) +
  # informar ao DD que b indica o bloco e atribuir 2 unidades tratadas em cada bloco
  declare_assignment(
    Z = conduct_ra(N = N, block_m = c(5, 5, 5), blocks = b),
    Z_cond_prob =
      obtain_condition_probabilities(assignment = Z, blocks = b, block_m = c(5, 5, 5))
  ) +
  # relação dos resultados potenciais com o resultado observado
  declare_potential_outcomes(Y ~ Z * y1 + (1 - Z) * y0) +
  # resultado observado e atribuição de tratamento
  declare_reveal(Y, Z)
```

## Qual estimador devemos usar?

```{r blockdd1, echo=TRUE, cache=TRUE}
# a estimativa é a média de efeito de tratamento
estimandATEb <- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))

# 3 estimadores diferentes
est1 <- declare_estimator(Y ~ Z,
  inquiry = estimandATEb, model = lm_robust,
  label = "Ignorando blocos"
)
est2 <- declare_estimator(Y ~ Z,
  inquiry = estimandATEb, model = difference_in_means, blocks = b,
  label = "DiM: Pesos por tamanho do bloco"
)
est3 <- declare_estimator(Y ~ Z,
  inquiry = estimandATEb, model = lm_robust,
  weights = (Z / Z_cond_prob) + ((1 - Z) / (Z_cond_prob)),
  label = "LM: Pesos por tamanho do bloco"
)
```

## Qual estimador devemos usar?

```{r blockdd1a, echo=TRUE, cache=TRUE}
# mais dois estimadores
est4 <- declare_estimator(Y ~ Z,
  inquiry = estimandATEb,
  model = lm_robust, fixed_effects = ~b, label = "Precision Weights"
)
est5 <- declare_estimator(Y ~ Z + b,
  inquiry = estimandATEb,
  model = lm_robust, label = "Precision Weights (LSDV)"
)

## O novo objeto de design terá o design base, a estimativa e cinco estimadores.
design_blocks <- base_design_blocks + estimandATEb +
  est1 + est2 + est3 + est4 + est5

trueATE_blocked <- draw_estimand(design_blocks)$estimand
```

Em seguida, iremos executar 10,000 simulações (reatribuir o tratamento
10,000 vezes) e resumir as estimativas produzidas por cada um desses
cinco estimadoress

## Qual estimador devemos usar?

```{r futurelibs, echo=FALSE}
## Isso habilita o processamento paralelo ao fazer os diagnósticos.
## Observe que ativamos o "cache" para alguns blocos de código para que eles não sejam
## re-executados toda vez que mudamos os slides.
library(future)
library(future.apply)
```

```{r diagnosis2, echo=FALSE, cache=TRUE}
## As próximas linhas utilizam todos os núcleos do seu computador para acelerar o cálculo.

plan(multicore)
set.seed(12345)
diagnosis2 <- diagnose_design(design_blocks, bootstrap_sims = 0, sims = 10000)
sims2 <- get_simulations(diagnosis2)
plan(sequential)
```

Como devemos interpretar esse gráfico?

```{r sim_plot2, message=FALSE, warning=FALSE, out.width=".9\\textwidth"}
sim_plot2 <- ggplot(sims2, aes(y = estimate, x = estimator, color = estimator)) +
  geom_boxplot() +
  geom_hline(yintercept = trueATE_blocked) +
  geom_point(aes(group = estimator)) +
  theme(
    text = element_text(size = 20), axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none", legend.title = element_blank()
  ) +
  xlab("")
print(sim_plot2)
```

## Qual estimador está mais próximo da verdade?

Qual estimador funciona melhor neste design e nestes dados?

```{r blocktab}
blocktab <- reshape_diagnosis(diagnosis2, select = c("Estimator", "Bias", "RMSE", "SD Estimate", "Power", "Coverage"))[, -c(1, 2, 4, 5, 6)]
kableExtra::kable(blocktab, col.names = c("Estimator", "Bias", "RMSE", "SD Est", "Power", "Coverage"))
```

Observe que a cobertura nem sempre é de 95% em todos os casos. Usamos
10,000 simulações, portanto, o erro de simulação é em torno de
$\pm 2 \sqrt{p(1-p)/10000}$ ou, digamos, para uma cobertura calculada
como 0,93, uma simulação diferente poderia facilmente ter produzido
`r .93 -2*sqrt( .93 * (1-.93)/10000 )` ou
`r .93+2*sqrt( .93 * (1-.93)/10000 )` (ou raramente teria produzido
números de cobertura fora desse intervalo apenas por acaso).

# Aleatorização por Agrupamentos (clusters)

## Em Experimentos Aleatorizados por Agrupamentos (clusters), as Unidades são randomizadas em clusters (agrupamentos) para atribuição do tratamento {.allowframebreaks}

-   **Exemplo 1:** uma intervenção é randomizada entre bairros, então
    **todas** as casas em um bairro serão designadas para a mesma
    condição de tratamento, mas diferentes bairros receberão diferentes
    condições de tratamento.

-   **Exemplo 2:** uma intervenção é randomizada entre as pessoas e cada
    pessoa é medida quatro vezes após o tratamento, então nossos dados
    contêm quatro linhas por pessoa.

-   **Não é um Exemplo 1:** Os bairros são escolhidos para o estudo.
    Dentro de cada bairro, cerca da metade das pessoas são designadas
    para tratamento e metade para controle. (Que tipo de estudo é este?
    Não é um estudo randomizado em clusters.)

-   **Não é um Exemplo 2:** uma intervenção é randomizada para alguns
    bairros e não para outros, os resultados incluem medições de
    confiança no governo em nível de bairro e a área total de terra no
    bairro dedicada a jardins. (Às vezes, um experimento randomizado em
    clusters pode ser transformado em um experimento randomizado
    simples. Ou pode conter mais de uma abordagem possível para análise
    e interpretação.)

Como a distribuição de estatísticas de teste e estimadores pode ser
diferente de um experimento em que as unidades individuais (não
clusters) são randomizadas?

## Estimando o ATE em Experimentos Aleatorizados por Agrupamentos (clusters)

Problemas de viés em experimentos randomizados em agrupamentos
(clusters):

-   Quando os agrupamentos (clusters) têm o mesmo tamanho, o estimador
    usual de diferença nas médias não é enviesado.

-   Mas tenha cuidado quando os agrupamentos (clusters) têm diferentes
    números de unidades ou quando há muito poucos agrupamentos
    (clusters), pois os efeitos do tratamento podem estar
    correlacionados com o tamanho do agrupamento (cluster).

-   Quando o tamanho do agrupamento (cluster) está relacionado aos
    resultados potenciais, o estimador usual de diferença nas médias é
    tendencioso.
    <https://declaredesign.org/blog/bias-cluster-randomized-trials.html>

## Estimando o Erro-padrão para o ATE em Experimentos Aleatorizados por Agrupamentos (clusters) {.allowframebreaks}

-   **Inferências estatísticas enganosas**: O erro padrão
    geralmente subestima a precisão em tais desenhos e, portanto, produz
    testes com taxas falsas de positivos que são muito altas (ou,
    equivalentemente, intervalos de confiança que têm cobertura muito
    baixa).

-   Os "erros padrão robustos agrupados", implementados em softwares
    comuns, funcionam bem **quando o número de agrupamentos (clusters) é
    grande** (como mais de 50 em alguns estudos de simulação).

-   Os erros padrão apropriados para clusters padrão em `lm_robust` (os
    erros `cr2`) funcionam melhor do que a abordagem comum no Stata (até
    o momento da escrita deste texto).

-   A bootstrapping ajuda a controlar as taxas de erro, mas
    perde muito mais poder estatístico do que talvez seja necessário em
    um estudo randomizado em clusters onde a inferência de randomização
    direta é possível.

-   Quando em dúvida, é possível produzir $p$-valores por simulação
    direta (inferência de randomização direta) para verificar se eles
    concordam com uma das abordagens robustas a clusters.

Em geral, vale a pena simular para estudar o desempenho de seus
estimadores, testes e intervalos de confiança se você tiver alguma
preocupação ou dúvida.

## Um Exemplo de Estimação

```{r makedatclus, echo=FALSE, results="hide"}
## veja https://declaredesign.org/blog/bias-cluster-randomized-trials.html para mais
N_clusters <- 10 # número de agrupamentos (clusters)
n_indivs <- c(100, 10) # possível tamanho dos agrupamentos (clusters)
thepop <- declare_population(
  clus_id = add_level(
    # determine o número de agrupamentos (clusters)
    N = N_clusters,
    # 1/5 dos agrupamentos (clusters) têm 100 indivíduos, 4/5 dos agrupamentos (clusters) têm 10 indivíduos
    cl_size = rep(n_indivs, c(N / 5, N - N / 5)),
    cl_sizeF = factor(cl_size),
    # Cada agrupamento (cluster) tem um nível diferente de média (u) e uma variação de contexto diferente (sd of u)
    effect = ifelse(cl_size == 100, .1, 1)
  ),
  indiv = add_level(
    N = cl_size,
    u = rnorm(N, mean = log(cl_size), sd = effect)
  )
)

theys <- declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + effect)

thetarget_indiv <- declare_inquiry(ATE_indiv = mean(Y_Z_1 - Y_Z_0))

## Atribuição aleatória completa para os agrupamenots (clusters)
theassign <- declare_assignment(Z = conduct_ra(N = N, clusters = clus_id))

thereveal <- declare_reveal(Y, Z)

## 7 estimadores diferentes
est1 <- declare_estimator(Y ~ Z,
  inquiry = "ATE_indiv", clusters = clus_id,
  model = lm_robust, label = "Y~Z, CR2 SE"
)

est2 <- declare_estimator(Y ~ Z,
  inquiry = "ATE_indiv",
  model = lm_robust, label = "Y~Z, HC2 SE"
)

est3 <- declare_estimator(Y ~ Z,
  inquiry = "ATE_indiv",
  model = lm_robust, se_type = "classical", label = "Y~Z, IID SE"
)

est4 <- declare_estimator(Y ~ Z + cl_sizeF,
  inquiry = "ATE_indiv", clusters = clus_id,
  model = lm_robust, label = "Y~Z+clus_size_fixed_effects, CR2 SE"
)

est5 <- declare_estimator(Y ~ Z,
  inquiry = "ATE_indiv", fixed_effects = ~cl_sizeF, clusters = clus_id,
  model = lm_robust, label = "Y~Z, fixed_effects=~clus_size_fixed_effects, CR2 SE "
)

est6 <- declare_estimator(Y ~ Z,
  inquiry = "ATE_indiv", covariates = ~cl_size, clusters = clus_id,
  model = lm_lin, label = "Y~Z*I(clus_size-mean(clus_size)), CR2 SE"
)

est7 <- declare_estimator(Y ~ Z,
  inquiry = "ATE_indiv", weight = cl_size, clusters = clus_id,
  model = lm_robust, label = "Y~Z, weight=clus_size, CR2 SE"
)

### Algumas coisas experimentais aqui:
## remotes::install_github("markmfredrickson/RItools",ref="proj1-balT")
## est7tmp <-  balanceTest(Z~Y+cluster(clus_id),data=dat1,report="all")
##
## est7fn <- function(data){
##     bt <- balanceTest(Z~Y+cluster(clus_id),data=data,report="all")
##     resdat <- data.frame(estimate=bt$results[,"adj.mean diff",])
##     return(resdat)
## }
## est7fn <- function(data){
##     thelme <- lmer(Y~Z+(1|clus_id),data=data)
##     cilme <- confint(thelme)
##     lmecoef <- summary(thelme)$coefficients["Z",]
##     resdat <- data.frame(estimate=lmecoef["Estimate"],
##         std.error=lmecoef["Std. Error"],
##             statistic=lmecoef["t value"],
##             p.value=NA,
##         conf.low=min(cilme["Z",]),
##         conf.high=max(cilme["Z",]))
##     return(resdat)
## }
##
## est7 <- declare_estimator(handler=tidy_estimator(est7fn),label="mlm: rand intercept")

des <- thepop + theys + theassign + thereveal

set.seed(12345)
dat1 <- draw_data(des)

head(dat1)

table(dat1$clus_id)
with(dat1, table(clus_id, Z))
dat1 %>%
  group_by(clus_id) %>%
  summarize(mean(Y_Z_1 - Y_Z_0))

## g1 <- ggplot(data=dat1,aes(x=Y,group=clus_id,fill=clus_id,color=clus_id))+
##     geom_density()
## g1

est1(dat1)
est2(dat1)
est3(dat1)
est4(dat1)
est5(dat1)
est6(dat1)
est7(dat1)
```

Imaginando que temos dados de 10 agrupamentos com 100 pessoas (para 2
clusters) ou 10 pessoas por agrupamento (para 8 clusters). O tamanho total
dos dados é `r nrow(dat1)`.

```{r}
tmp <- dplyr::filter(dat1, clus_id %in% c("03", "01")) %>%
  group_by(clus_id) %>%
  sample_n(3) %>%
  arrange(clus_id, indiv) %>%
  select(clus_id, indiv, Y_Z_0, Y_Z_1, Z, Y)

tmp
```

## Exemplo de Estimação

Qual estimador devemos usar? A qual teste devemos recorrer? Com base em que
devemos escolher entre essas abordagens?

```{r clusest, echo=TRUE}
lmc1 <- lm_robust(Y ~ Z, data = dat1)
lmc2 <- lm_robust(Y ~ Z, clusters = clus_id, data = dat1)
lmc3 <- lm_robust(Y ~ Z + cl_sizeF, clusters = clus_id, data = dat1)
tidy(lmc1)[2, ]
tidy(lmc2)[2, ]
tidy(lmc3)[2, ]
```

## Usando Simulação para Avaliar Estimadores e Testes

Se você olhar o código dos slides, verá que simulamos o desenho 5,000
vezes, calculando cada vez uma estimação e intervalo de confiança para
diferentes estimadores do ATE.

O que devemos aprender com esta tabela? (Cobertura? `sd_estimate` versus
`mean_se`).

```{r simdesign, warning=FALSE, results="hide"}
des_plus_est <- des + thetarget_indiv + est1 + est2 + est3 + est4 + est5 + est6 + est7
des_plus_est
```

```{r diag_clust, cache=TRUE, warning=FALSE, message=FALSE}
set.seed(12346)
plan(multicore)
diag_clus <- diagnose_design(des_plus_est, bootstrap_sims = 0, sims = 1000)
sim_clus <- get_simulations(diag_clus) # simulate_design(des_plus_est,sims=1000)
trueclusATE <- thetarget_indiv(dat1)[["estimando"]]
plan(sequential)
```

```{r cluster_sim_res}
## Observe que o estimador linear é ótimo, mas às vezes não consegue produzir uma resposta.
res_clus <- sim_clus %>%
  na.omit() %>%
  group_by(estimator) %>%
  summarize(
    bias = mean(estimate - estimand),
    rmse = sqrt(mean((estimate - estimand)^2)),
    power = mean(p.value < .05),
    coverage = mean(estimand <= conf.high & estimand >= conf.low),
    # mean_estimate = mean(estimate),
    sd_estimate = sd(estimate),
    mean_se = mean(std.error)
  )
res_clus[2, "estimador"] <- "Y~Z, cl_size fe, CR2"
res_clus[6, "estimador"] <- "Y~Z*I(cl_size-mean(cl_size)), CR2"
res_clus[7, "estimador"] <- "Y~Z+cl_sizeF, CR2"
res_clus$estimator <- gsub(" SE", "", res_clus$estimator)
```

```{r showresclus1}
kableExtra::kable(res_clus[, c(1, 5:7)], digits = 2, booktabs = TRUE, linesep = "", caption = "Estimator and Test Performance in 5000 simulations of the cluster randomized design for different estimators and confidence intervals")
```

## Usando Simulação para Avaliar Estimadores e Testes

O que devemos aprender com essa tabela? (Viés? Proximidade à verdade?)

```{r showresclus2}
kableExtra::kable(res_clus[, c(1:3)], digits = 3, booktabs = TRUE, linesep = "", caption = "Desempenho do Estimador e Teste em 5000 simulações do desenho randomizado por cluster para diferentes estimadores e intervalos de confiança.")
```

## Usando Simulação para Avaliar Estimadores e Testes

Como devemos interpretar esse gráfico?

```{r sim_plot_clus, warning=FALSE, out.width=".95\\textwidth"}
sim_plot3 <- ggplot(sim_clus, aes(y = estimate, x = estimator, color = estimator)) +
  geom_boxplot() +
  coord_flip() +
  geom_hline(yintercept = trueclusATE) +
  geom_point(aes(group = estimator)) +
  theme(
    text = element_text(size = 20),
    legend.position = "none", legend.title = element_blank()
  ) +
  ylab("")
print(sim_plot3)
```

## Resumo da Estimação e dos Testes de Avaliações Aleatórias por Agrupamento (-clusterss)

-   Avaliações randomizados agrupadas (clusterizadas) apresentam
    problemas especiais para abordagens padrão de estimativa e teste.

-   Se a randomização for no nível do agrupamento (cluster), incerteza
    surge a partir da randomização no nível do agrupamento (cluster).

-   Se tivermos agrupamentos suficientes, então um dos erros padrão
    "robustos por agrupamento" pode nos ajudar a produzir intervalos de
    confiança com cobertura correta. **Erros padrão robustos agrupado
    (clusterizado) requerem muitos agrupamentos (clusters).**

-   Se o tamanho (ou característica) do agrupamento (cluster) estiver
    relacionado ao tamanho do efeito, então podemos ter viés (e
    precisamos ajustar de alguma forma).

# Resultados Binários

## Resultados Binários: Configure os Dados para Simulação no `DeclareDesign`

```{r setupbin, echo=TRUE}
# tamanho da população
N <- 20
# identifique a população
thepop_bin <- declare_population(
  N = N, x1 = draw_binary(prob = .5, N = N),
  x2 = rnorm(N)
)
# identifique os resultados potenciais
thepo_bin <- declare_potential_outcomes(Y ~ rbinom(
  n = N, size = 1,
  prob = 0.5 + 0.05 * Z + x1 * .05
))
# dois possíveis objetivos: diferença nas médias ou diferença nos log-odds
thetarget_ate <- declare_inquiry(ate = mean(Y_Z_1 - Y_Z_0))
thetarget_logodds <- declare_inquiry(
  logodds = log(mean(Y_Z_1) / (1 - mean(Y_Z_1))) -
    log(mean(Y_Z_0) / (1 - mean(Y_Z_0)))
)
```

## Resultados Binários: Configure os Dados para Simulação no `DeclareDesign`

```{r setupbin2, echo=TRUE}
# declarar como o tratamento é atribuído
# m unidades são atribuídas a níveis de tratamento Z
theassign_bin <- declare_assignment(Z = conduct_ra(N = N, m = floor(N / 3)))
# declarar quais valores de resultado são revelados para possíveis valores de Z
thereveal_bin <- declare_reveal(Y, Z)
# juntar tudo isso: população, resultados potenciais, atribuição,
# valores de resultado conectados a Z
des_bin <- thepop_bin + thepo_bin + theassign_bin + thereveal_bin
# então faça uma única seleção aleatória (tratamento aleatório uma vez)
set.seed(12345)
dat2 <- draw_data(des_bin)
```

## Resultados Binários: Estimativas I

Os dados simulados: `Y_Z_1`, `Y_Z_0` são resultados potenciais, `Y` é
observado, `x1`, `x2` são covariáveis e `Z` é a atribuição de
tratamento. Aqui $N$=`r nrow(dat2)`.

```{r dat2echo, echo=TRUE}
## Olhando para apenas as 6 primeiras observações:
head(dat2[, -7])
```

## Resultados Binários: Estimativas II

Como interpretar as seguintes quantidades ou estimativas verdadeiros?
(`Y_Z_1`, `Y_Z_0` são potenciais resultados, `Y` é observado, `x1` e
`x2` são covariáveis, `Z` é a atribuição de tratamento. Aqui
$N$=`r nrow(dat2)`.

```{r bin1, echo=TRUE}
ate_bin <- with(dat2, mean(Y_Z_1 - Y_Z_0))
bary1 <- mean(dat2$Y_Z_1)
bary0 <- mean(dat2$Y_Z_0)
diff_log_odds_bin <- with(
  dat2,
  log(bary1 / (1 - bary1)) - log(bary0 / (1 - bary0))
)
c(
  bary1 = bary1, bary0 = bary0, true_ate = ate_bin,
  true_diff_log_odds = diff_log_odds_bin
)
```

## Resultados Binários: Estimativas III

Quer estimar as diferenças entre os log-odds?

```{=tex}
\begin{equation}
\delta = \log \frac{\bar{y}_{1}}{1-\bar{y}_{1}} - \log \frac{ \bar{y}_0}{1- \bar{y}_0}
\end{equation}
```
Ou a diferença entre as proporções?

```{=tex}
\begin{equation}
\bar{\tau} = \bar{y}_{1} - \bar{y}_0
\end{equation}
```
Lembre-se que $\bar{y}_1$ é a proporção de $y_{1}=1$ nos dados.

@freedman2008randomization mostra que o estimador do coeficiente de
logit é um estimador enviesado da estimativa da diferença em log-odds.
Ele também mostra um estimador não-enviesado dessa estimativa.

Sabemos que a diferença de proporções na amostra deve ser um estimador
não-enviesado da diferença de proporções.

## Exemplo de Estimação I

Como devemos interpretar as seguintes estimativas? (O que o estimador da
diferença de médias exige em termos de suposições? O que o estimador de
regressão logística exige em termos de suposições?)

```{r estexample, echo=TRUE}
lmbin1 <- lm_robust(Y ~ Z, data = dat2)
glmbin1 <- glm(Y ~ Z, data = dat2, family = binomial(link = "logit"))

tidy(lmbin1)[2, ]
tidy(glmbin1)[2, ]
```

## Exemplo de Estimação II

E quanto aos covariáveis? Por que usar covariáveis?

```{r estexample2, echo=TRUE}
lmbin2 <- lm_robust(Y ~ Z + x1, data = dat2)
glmbin2 <- glm(Y ~ Z + x1, data = dat2, family = binomial(link = "logit"))

tidy(lmbin2)[2, ]
tidy(glmbin2)[2, ]
```

## Exemplo de Estimação III

Vamos comparar nossas estimativas

```{r estexample3, echo=TRUE}
c(
  dim = coef(lmbin1)[["Z"]],
  dim_x1 = coef(lmbin2)[["Z"]],
  glm = coef(glmbin1)[["Z"]],
  glm_x1 = coef(glmbin2)[["Z"]]
)
```

## Um Exemplo de Estimação: Os Estimadores *Plugin* de Freedman I

Sem covariável:

```{r pluginest, echo=TRUE }
freedman_plugin_estfn1 <- function(data) {
  glmbin <- glm(Y ~ Z, data = dat2, family = binomial(link = "logit"))
  preddat <- data.frame(Z = rep(c(0, 1), nrow(dat2)))
  preddat$yhat <- predict(glmbin, newdata = preddat, type = "response")
  bary1 <- mean(preddat$yhat[preddat$Z == 1])
  bary0 <- mean(preddat$yhat[preddat$Z == 0])
  diff_log_odds <- log(bary1 / (1 - bary1)) - log(bary0 / (1 - bary0))
  return(data.frame(estimate = diff_log_odds))
}
```

## Um Exemplo de Estimação: Os Estimadores *Plugin* de Freedman II

Com covariável:

```{r pluginest2, echo=TRUE }
freedman_plugin_estfn2 <- function(data) {
  N <- nrow(data)
  glmbin <- glm(Y ~ Z + x1, data = data, family = binomial(link = "logit"))
  preddat <- data.frame(Z = rep(c(0, 1), each = N))
  preddat$x1 <- rep(data$x1, 2)
  preddat$yhat <- predict(glmbin, newdata = preddat, type = "response")
  bary1 <- mean(preddat$yhat[preddat$Z == 1])
  bary0 <- mean(preddat$yhat[preddat$Z == 0])
  diff_log_odds <- log(bary1 / (1 - bary1)) - log(bary0 / (1 - bary0))
  return(data.frame(estimate = diff_log_odds))
}
```

Vamos comparar nossas estimativas dos seis diferentes estimadores.

```{r echo=FALSE}
c(
  dim = coef(lmbin1)[["Z"]],
  dim_x1 = coef(lmbin2)[["Z"]],
  glm = coef(glmbin1)[["Z"]],
  glm_x1 = coef(glmbin2)[["Z"]],
  freedman = freedman_plugin_estfn1(dat2)[["estimate"]],
  freeman_x1 = freedman_plugin_estfn2(dat2)[["estimate"]]
)
```

```{r tmleapproach, eval=FALSE}
## Aqui está outra abordagem para usar o estimador do plugin, mas permitindo erros padrão, etc.
## Não requer uma função escrita à mão como as usadas acima.
library(tmle)
Y <- as.matrix(dat2$Y, ncol = 1)
A <- as.matrix(dat2$Z, ncol = 1)
W <- as.matrix(dat2[, c("x1", "x2")], ncol = 1)
colnames(W) <- paste("W", 1:2, sep = "")
tmle1 <- tmle(
  Y = Y, A = A, W = W,
  family = "binomial", Qform = Y ~ A, gform = A ~ 1, cvQinit = FALSE,
  Q.SL.library = "SL.glm", g.SL.library = "SL.glm", g.Delta.SL.library = "SL.glm"
)

tmle1$estimates$ATE
tmle1$estimates$OR

tmle2 <- tmle(
  Y = Y, A = A, W = W,
  family = "binomial", Qform = Y ~ A + W1, gform = A ~ 1, cvQinit = FALSE, # V=0,
  Q.SL.library = "SL.glm", g.SL.library = "SL.glm", g.Delta.SL.library = "SL.glm"
)

tmle2$estimates$ATE
tmle2$estimates$OR
```

## Um Exemplo de Uso do `DeclareDesign` para Avaliar Nossos Estimadores I

```{r ddbinsetup, echo=TRUE}
# declara 4 estimadores para o DD
# primeiro estimador: regressão linear com o ATE como objetivo
estb1 <- declare_estimator(Y ~ Z,
  model = lm_robust, label = "lm1:Z",
  inquiry = thetarget_ate
)
# segundo estimador: regressão linear com covariável, com o ATE como objetivo
estb2 <- declare_estimator(Y ~ Z + x1,
  model = lm_robust, label = "lm1:Z,x1",
  inquiry = thetarget_ate
)
# terceiro estimador: regressão logística, com log de chances como objetivo
estb3 <- declare_estimator(Y ~ Z,
  model = glm, family = binomial(link = "logit"),
  label = "glm1:Z", inquiry = thetarget_logodds
)
# quarto estimador: regressão logística com covariável, com log de chances como objetivo
estb4 <- declare_estimator(Y ~ Z + x1,
  model = glm, family = binomial(link = "logit"),
  label = "glm1:Z,x1", inquiry = thetarget_logodds
)
```

## Um Exemplo de Uso do `DeclareDesign` para Avaliar Nossos Estimadores II

```{r ddbinsetup2, echo=TRUE}
# Junte tudo: des_bin é a população, resultados potenciais, atribuição de tratamento, valores de resultados conectados a Z. Adicionamos os dois alvos e quatro estimadores.
des_bin_plus_est <- des_bin + thetarget_ate + thetarget_logodds +
  estb1 + estb2 + estb3 + estb4
```

```{r diagnosis_bin, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
## As próximas linhas usam todos os núcleos do seu computador para acelerar o cálculo.
library(future)
library(future.apply)
# plan(multiprocess)
set.seed(12345)
diagnosis_bin <- diagnose_design(des_bin_plus_est, bootstrap_sims = 0, sims = 1000)
sims_bin <- get_simulations(diagnosis_bin)
trueATE_bin <- thetarget_ate(dat2)[["estimand"]]
truelo_bin <- thetarget_logodds(dat2)[["estimand"]]
# plan(sequential)
```

## Usando Simulação Para Avaliar Nossos Estimadores

Como devemos interpretar este gráfico? (Diferenças nas escalas tornam
isso difícil.)

```{r sim_plot_bin, out.width=".95\\textwidth"}
estimand_dat <- sims_bin %>%
  group_by(inquiry) %>%
  summarize(meanestimand = mean(estimand))
sim_plot_bin <- ggplot(sims_bin, aes(y = estimate, x = estimator, color = estimator)) +
  geom_boxplot() +
  geom_point() +
  facet_wrap(~inquiry, scales = "free") +
  geom_hline(data = estimand_dat, aes(yintercept = meanestimand)) +
  theme(
    text = element_text(size = 20),
    legend.position = "none", legend.title = element_blank()
  )
print(sim_plot_bin)
```

## Qual Estimador Está Mais Próximo da Verdade

Qual estimador funciona melhor neste design e nesses dados?

```{r bin_sim_res}
## Observe que o estimador "lin" é ótimo, mas às vezes não consegue produzir uma resposta
res_bin <- sims_bin %>%
  group_by(estimator, inquiry) %>%
  summarize(
    bias = mean(estimate - estimand),
    rmse = sqrt(mean((estimate - estimand)^2)),
    power = mean(p.value < .05),
    coverage = mean(estimand <= conf.high & estimand >= conf.low, na.rm = TRUE),
    # mean_estimate = mean(estimate),
    sd_est = sd(estimate),
    mean_se = mean(std.error)
  )
names(res_bin)[1:2] <- c("est", "estimand")
```

```{r showresbin1}
kableExtra::kable(res_bin, digits = 3, booktabs = TRUE, linesep = "", caption = "Desempenho dos estimadores e testes em 5,000 simulações para diferentes estimadores e intervalos de confiança para um resultado binário e um desenho completamente randomizado.")
```

# Outros Tópicos em Estimativa

## Ajuste de Covariância: Estimandos

Em geral, simplesmente "controlar por" produz um estimador enviesado da
estimativa do ATE **ou** dp ITT. Veja, por exemplo, @lin_agnostic_2013 e
@freedman2008rae. @lin_agnostic_2013 mostra como reduzir esse viés e, o
que é importante, que esse viés tende a ser pequeno à medida que o
tamanho da amostra aumenta.

# Conclusão

## Considerações Finais Sobre os Fundamentos da Estimação

-   Estimativas causais contrafactuais são funções não observadas de
    resultados potenciais.

-   Estimadores são receitas ou fórmulas computacionais que usam dados
    observados para aprender sobre uma estimativa.

-   Bons estimadores produzem estimações que são próximas da
    verdadeira estimativa.

-   (Conectando a estimativa com o teste) Os erros padrão dos
    estimadores nos permitem calcular intervalos de confiança e
    $p$-valores. Certos estimadores têm erros padrão maiores ou menores
    (ou mais ou menos corretos).

-   Você pode avaliar a utilidade de um estimador escolhido para um
    estimando escolhido por simulação.

# Efeitos Causais que Diferem por Grupos ou Covariáveis

## Efeitos que Diferem por Grupos I

Se nossa teoria sugere que os efeitos devem variar por grupo,
como podemos avaliar evidências a favor ou contra tais afirmações?

-   Podemos **desenhar** uma avaliação dessa teoria criando um estudo
    com randomização por blocos --- com blocos definidos pelos grupos
    teoricamente relevantes.

-   Podemos **planejar** essa avaliação através de (1) **pré-registrar
    análises de subgrupos específicos** (seja ou não bloco no grupo na
    fase de design) e (2) garantir que a adesão ao grupo seja medida
    durante a coleta de dados de linha de base pré-tratamento

## Efeitos que Diferem por Grupos II

-   Se não planejarmos com antecedência, as análises específicas de
    subgrupo podem ser úteis como explorações, mas não devem ser
    consideradas confirmatórias: elas podem criar facilmente problemas
    de testar muitas hipóteses, inflando as taxas de falsos-positivos.

-   Não **devemos usar grupos formados pelo tratamento**. (Isso é
    chamado de "análise de mediação" ou "condicionamento em variáveis
    pós-tratamento" e merece seu próprio módulo).

# Efeitos Causais Quando Não Controlamos a Dose

## Definindo Efeitos Causais I

Imagine um experimento de comunicação porta-a-porta, onde algumas casas
são atribuídas aleatoriamente para receber uma visita. Note que agora
usamos $Z$ e $d$ em vez de $T$.

-   $Z_i$ é a atribuição aleatória da visita ($Z_i=1$) ou
    não ($Z_i=0$).
-   $d_{i,Z_i=1}=1$ significa que a pessoa $i$ abriria a porta para
    conversar quando atribuída uma visita.
-   $d_{i,Z_i=1}=0$ significa que a pessoa $i$ não abriria a porta para
    conversar quando atribuída uma visita.
-   Abrir a porta é um resultado do tratamento.

```{=tex}
\begin{center}
\begin{tikzcd}[ampersand replacement=\&]
Z  \arrow[from=1-1,to=1-2, "\ne 0"] \arrow[from=1-1, to=1-4, bend left, "\text{0 (exclusion)}"] \& d  \arrow[from=1-2,to=1-4] \& \& y \\
(x_1 \ldots x_p) \arrow[from=2-1,to=1-1, "\text{0 (as if randomized)}"]  \arrow[from=2-1,to=1-2] \arrow[from=2-1,to=1-4]
\end{tikzcd}
\end{center}
```
## Definindo Efeitos Causais II

-   $y_{i,Z_i = 1, d_{i,Z_i=1}=1}$ é o resultado potencial para pessoas
    que foram atribuídas uma visita e que abriram a porta.
    ("Cumpridores" ou "Sempre respondem")

-   $y_{i,1, d_{i,Z_i=1}=0}$ é o resultado potencial para pessoas que
    foram atribuídas a uma visita e que não abriram a porta. ("Nunca
    respondem" ou "Desafiadores")

-   $y_{i,0, d_{i,0}=1}$ é o resultado potencial para pessoas que não
    foram atribuídas a uma visita e que abririam a porta.
    ("Desafiadores" ou "Sempre respondem")

-   $y_{i,0, d_{i,0}=0}$ é o resultado potencial para pessoas que não
    foram atribuídas a uma visita e que não abririam a porta.
    ("Cumpridores" ou "Nunca respondem")

## Definindo Efeitos Causais III

Também podemos defininr $y_{i,Z_i = 0, d_{i,Z_i=1}=1}$ para pessoas que não foram
atribuídas uma visita, mas que teriam aberto a porta se tivessem sido
atribuídas uma visita, etc.

Nesse caso, podemos simplificar nossos resultados potenciais:

-   $y_{i,0, d_{i,1}=1} = y_{i,0, d_{i,1}=0} = y_{i,0, d_{i,0}=0}$
    porque o resultado é o mesmo independentemente de como você não abre
    a porta.

## Definindo Efeitos Causais IV

Podemos simplificar as maneiras pelas quais as pessoas recebem uma dose
do tratamento da seguinte forma (onde $d$ está em letra minúscula
refletindo a ideia de que se você abre a porta quando visitado ou não é
uma característica fixa como um resultado potencial):

-   $Y$ : resultado ($y_{i,Z}$ or $y_{i,Z_i=1}$ para o resultado
    potencial do tratamento para a pessoa $i$, fixo)
-   $X$ : covariável/variável de linha de base
-   $Z$ : atribuição de tratamento ($Z_i=1$ se atribuído a uma visita,
    $Z_i=0$ se não atribuído a uma visita)
-   $D$: tratamento recebido ($D_i=1$ se atender o telefone, $D_i=0$ se
    a pessoa $i$ não abrir a porta) (usando $D$ aqui porque
    $D_i = d_{i,1} Z_{i} + d_{i,0} (1-Z_i)$)

## Definindo Efeitos Causais V

Temos dois efeitos causais de $Z$: $Z \rightarrow Y$ ($\delta$, ITT,
ITT$_Y$), e $Z \rightarrow D$ (GG chama isso de ITT$_D$).

E diferentes tipos de pessoas podem reagir de maneira diferente à
tentativa de mover a dosagem com o instrumento.

```{=tex}
\centering
\begin{tabular}{llcc}
                       &        & \multicolumn{2}{c}{$Z=1$} \\
               &       & $D=0$ & $D=1$ \\
               \midrule
\multirow{2}{*}{$Z=0$} & $D=0$ & Never taker & Complier \\
                       & $D=1$ & Defier     & Always taker \\
               \bottomrule
\end{tabular}
```
## Definindo Efeitos Causais VI

O $ITT=ITT_Y=\delta= \bar{y}_{Z=1} - \bar{y}_{Z=0}$.

\medskip

Mas, neste desenho, $\bar{y}{Z=1}=\bar{y}{1}$ é dividido em partes: o
resultado daqueles que atenderam à porta (Compliers, Always-takers e
Defiers). Escreva $p_C$ para a proporção de compliers no estudo.

```{=tex}
\begin{equation}
\bar{y}_{1}=(\bar{y}_{1}|C)p_C + (\bar{y}_{1}|A)p_A + (\bar{y}_1|N)p_N + (\bar{y}_1|D)p_D.
\end{equation}
```
E $\bar{y}_{0}$ também é dividido em partes:

```{=tex}
\begin{equation}
\bar{y}_{0}=(\bar{y}_{0}|C)p_C + (\bar{y}_{1}|A)p_A + (\bar{y}_{0}|N)p_N + (\bar{y}_0|D)p_D.
\end{equation}
```

## Definindo Efeitos Causais VII

Então, o ITT em si é uma combinação dos efeitos de $Z$ em $Y$ dentro
desses diferentes subgrupos (imagine substituindo e rearranjando para que
tenhamos um conjunto de ITTs, um para cada tipo de sujeito). No entanto,
ainda podemos estimá-lo porque temos estimadores imparciais de
$\bar{y}_1$ e $\bar{y}_0$ dentro de cada tipo.

## Aprendendo sobre o ITT I

Primeiro, vamos aprender sobre o efeito da própria política. Para
escrever o ITT, não precisamos considerar todos os tipos mencionados
acima. Não temos defiers ($p_D=0$) e sabemos que o ITT para ambos
Always-takers e Never-takers é 0.

```{=tex}
\begin{equation}
\bar{y}_{1}=(\bar{y}_{1}|C)p_C + (\bar{y}_{1}|A)p_A + (\bar{y}_1|N)p_N
\end{equation}
```
```{=tex}
\begin{equation}
\bar{y}_{0}=(\bar{y}_{0}|C)p_C + (\bar{y}_{0}|A)p_A + (\bar{y}_{0}|N)p_N
\end{equation}
```
## Aprendendo sobre o ITT II

Vamos aprender sobre o efeito da própria política. Para escrever o ITT,
não precisamos considerar todos os tipos acima. Não temos desafiadores
($p_D=0$) e sabemos que o ITT tanto para Always-takers quanto para Never-taker é 0.

```{=tex}
\begin{align}
ITT    = & \bar{y}_{1} - \bar{y}_{0} \\
        = & ( (\bar{y}_{1}|C)p_C + (\bar{y}_{1}|A)p_A + (\bar{y}_1|N)p_N ) - \\
       & ( (\bar{y}_{0}|C)p_C + (\bar{y}_{0}|A)p_A + (\bar{y}_{0}|N)p_N )  \\
       \intertext{collecting each type together --- to have an ITT for each type}
       = & ( (\bar{y}_{1}|C)p_C -  (\bar{y}_{0}|C)p_C )  +   ( (\bar{y}_{1}|A)p_A - (\bar{y}_{1}|A)p_A ) + \\
       & ( (\bar{y}_1|N)p_N  - (\bar{y}_{0}|N)p_N ) \\
       = & \left( (\bar{y}_{1}|C) -  (\bar{y}_{0}|C) \right)p_C   +  \\
       & \left( (\bar{y}_{1}|A)- (\bar{y}_{0}|A) \right)p_A  +  \left( (\bar{y}_1|N) - (\bar{y}_{0}|N) \right)p_N
\end{align}
```
## Aprendendo sobre o ITT III

```{=tex}
\begin{align}
ITT     = &   \bar{y}_{1} - \bar{y}_{0} \\
        = &  ( (\bar{y}_{1}|C)p_C + (\bar{y}_{1}|A)p_A + (\bar{y}_1|N)p_N ) - \\
       & ( (\bar{y}_{0}|C)p_C + (\bar{y}_{0}|A)p_A + (\bar{y}_{0}|N)p_N )  \\
        = &   ( (\bar{y}_{1}|C)p_C -  (\bar{y}_{0}|C)p_C )  +   ( (\bar{y}_{1}|A)p_A - (\bar{y}_{1}|A)p_A ) + \\
       & ( (\bar{y}_1|N)p_N  - (\bar{y}_{0}|N)p_N ) \\
        = &   ( (\bar{y}_{1}|C) -  (\bar{y}_{0}|C))p_C   +   ( (\bar{y}_{1}|A)- (\bar{y}_{0}|A))p_A  + \\
       & ( (\bar{y}_1|N) - (\bar{y}_{0}|N) )p_N
\end{align}
```
## Aprendendo sobre o ITT IV

E, se o efeito da dose só pode ocorrer para aqueles que abrem a porta, e
você só pode abrir a porta quando designado para fazê-lo, então:

```{=tex}
\begin{equation}
( (\bar{y}_{1}|A)- (\bar{y}_{0}|A))p_A = 0  \text{ and } ( (\bar{y}_1|N) - (\bar{y}_{0}|N) )p_N = 0
\end{equation}
```

E

```{=tex}
\begin{equation}
ITT =  ( (\bar{y}_{1}|C) -  (\bar{y}_{0}|C))p_C  = ( CACE ) p_C.
\end{equation}
```

## O Efeito Causal Médio do Cumpridor Nato (complier) I

Também gostaríamos de aprender sobre o efeito causal de atender a porta
e ter a conversa, que é o efeito de interesse teórico.

Mas essa comparação é confundida por $x$: uma simples comparação de
$\bar{Y}|D=1 - \bar{Y}|D=0$ nos informa sobre as diferenças no resultado
devido a $x$ além da diferença causada por $D$. (Os números abaixo são
de alguns dados simulados)

```{=tex}
\begin{center}
\begin{tikzcd}[ampersand replacement=\&]
Z  \arrow[from=1-1,to=1-2] \arrow[from=1-1, to=1-4, bend left, "\text{0 (exclusion)}"] \& D  \arrow[from=1-2,to=1-4] \& \& y \\
(x_1 \ldots x_p) \arrow[from=2-1,to=1-1, "\text{-.006 (as if randomized)}"]  \arrow[from=2-1,to=1-2, ".06"] \arrow[from=2-1,to=1-4, ".48"]
\end{tikzcd}
\end{center}
```

## O Efeito Causal Médio do Cumpridor Nato (complier) II

```{r cors, eval=FALSE, echo=TRUE, results="hide"}
with(dat, cor(Y, x)) ## pode ser qualquer número
with(dat, cor(d, x)) ## pode ser qualquer número
with(dat, cor(Z, x)) ## deve ser próximo de 0
```

Sim, acabamos de ver que, nesse desenho e com essas suposições
(incluindo a suposição do SUTVA), que
$ITT = ( (\bar{y}_{1}|C) - (\bar{y}_{0}|C))p_C = (CACE) p_C$, então
podemos definir $CACE=ITT/p_C$.

## Como calcular o ITT e CACE/LATE I

```{r simivdesign, echo=FALSE}
prob_comply <- .8
tau <- .5

the_pop <- declare_population(
  N = 100,
  X = sample(1:4, N, replace = TRUE),
  u = rnorm(N),
  type = sample(c("Always-Taker", "Never-Taker", "Complier", "Defier"), N,
    replace = TRUE,
    prob = c(.1, 1 - unique(prob_comply), unique(prob_comply), 0)
  )
)

##  Os resultados potenciais não observados, Y(Z=1) e Y(Z=0), se relacionam com o resultado observado, Y, por meio da atribuição de tratamento e um efeito aditivo constante de tau.
## D se refere a receber uma dose de feedback.
d_po <- declare_potential_outcomes(
  D ~ case_when(
    Z == 0 & type %in% c("Never-Taker", "Complier") ~ 0,
    Z == 1 & type %in% c("Never-Taker", "Defier") ~ 0,
    Z == 0 & type %in% c("Always-Taker", "Defier") ~ 1,
    Z == 1 & type %in% c("Always-Taker", "Complier") ~ 1
  )
)

y_po <- declare_potential_outcomes(
  Y ~ tau * sd(u) * D + u,
  assignment_variables = c("D", "Z")
)

## A atribuição de tratamento para qualquer cidade específica é uma proporção fixa simples. Deve ser uma atribuição completa ou de sorteio de urna, não uma atribuição simples ou por lançamento de moeda.
## theassign <- declare_assignment(m=m)
the_assign <- declare_assignment(Z = complete_ra(N))

## declare_reveal é basicamente o mesmo que declare_potential_outcomes. Acredito que eles o tenham aqui para lidar com situações de dados ausentes ou não conformidade.
# thereveal <- declare_reveal(Y, Z)
d_reveal <- declare_reveal(D, assignment_variable = "Z")
y_reveal <- declare_reveal(Y, assignment_variables = c("D", "Z"))

base_design <- the_pop + the_assign + d_po + y_po + d_reveal + y_reveal

dat0 <- draw_data(base_design)

estimand_cace <- declare_inquiry(
  CACE = mean((Y_D_1_Z_1 + Y_D_1_Z_0) / 2 -
    (Y_D_0_Z_1 + Y_D_0_Z_0) / 2),
  subset = (type == "Complier")
)
estimand_ate <- declare_inquiry(ATE = mean((Y_D_1_Z_1 + Y_D_1_Z_0) / 2 -
  (Y_D_0_Z_1 + Y_D_0_Z_0) / 2))
```

Algumas amostras de dados (onde conhecemos todos os resultados potenciais):

```{r showdat0}
tempdat <- dat0[1:2, -1]
## Altere os nomes para tornar a tabela abaixo mais fácil de ler.
names(tempdat)[5] <- "pZ"
names(tempdat) <- gsub("_", "", names(tempdat))
kableExtra::kable(tempdat, digits = 2)
```

## Como calcular o ITT e CACE/LATE II

O ITT e CACE (em partes)

```{r echo=TRUE}
itt_y <- difference_in_means(Y ~ Z, data = dat0)
itt_y
itt_d <- difference_in_means(D ~ Z, data = dat0)
itt_d
```

## Como calcular o ITT e CACE/LATE III

Todos juntos:[^1]

[^1]: funciona quando $Z \rightarrow D$ não é fraco. Ver @imbens2005robust para mais detalhes

```{r echo=TRUE}
cace_est <- iv_robust(Y ~ D | Z, data = dat0)
cace_est
## Notice same as below:
coef(itt_y)[["Z"]] / coef(itt_d)[["Z"]]
```

## Resumo dos desenhos orientados por Encorajamento/Cumpridor Nato (Complier)/Dose:

-   Analise como você randomizou, mesmo quando você não controla a dose.
-   Há um perigo na análise por protocolo.

## Referências
